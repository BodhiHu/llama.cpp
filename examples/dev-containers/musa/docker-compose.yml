version: '3'
services:
  llama.cpp:
    build:
      context: ../../..
      dockerfile: docker/musa/Dockerfile
    volumes:
      - ../../..:/app
      - ../../../../models/:/models
    restart: always
    environment:
      - HOST=0.0.0.0
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #       - driver: musa
    #         # use all GPUs or the specified ones:
    #         # device_ids: ['0','1','2','3']
    #         capabilities: [gpu]
