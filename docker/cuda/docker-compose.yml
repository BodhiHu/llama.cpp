version: '3'
services:
  cuda-llama.cpp:
    build:
      context: ../..
      dockerfile: docker/cuda/Dockerfile.cuda
    volumes:
      - ../..:/app
      - ../../._ollama:/root.ollama
    #ports:
    #  - 16888:16888
    network_mode: host
    command: ./_apps/ollama/ollama serve
    # command: ./server-mm -m examples/llava/phi3-models/gguf/ggml-model-f16.gguf --mmproj examples/llava/phi3-models/gguf/vit/mmproj-model-f16.gguf -c 4096 --host 0.0.0.0 --port 8080
    restart: always
    environment:
      - OLLAMA_HOST=0.0.0.0:16888
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            # use all GPUs or the specified ones:
            # device_ids: ['0','1','2','3']
            device_ids: ['0']
            capabilities: [gpu]
